"""

Converts .csv with partitioned dataset generated by aisi_joints.data.partition_dataset to .tfrecord
files required by TFOD.

Run as `python -m aisi_joins.data.generate_tfrecord -h` for instructions.
"""

import io
import logging
import os
import os.path as path
from argparse import ArgumentParser, Namespace
from typing import NamedTuple, Dict

import pandas as pd
import tensorflow as tf
import tqdm
from PIL import Image
from object_detection.utils import dataset_util
from object_detection.utils import label_map_util

from .utils import generate_class_weights

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging (1)


class Sample(NamedTuple):
    eventId: str
    x0: int
    x1: int
    y0: int
    y1: int
    cls: str
    filepath: str


log = logging.getLogger(__name__)


def read_tfrecord(example: tf.train.Example) -> dict:
    tfrecord_format = (
        {
            'image/height': tf.io.FixedLenFeature([], tf.int64),
            'image/width': tf.io.FixedLenFeature([], tf.int64),
            'image/filename': tf.io.FixedLenFeature([], tf.string),
            'image/source_id': tf.io.FixedLenFeature([], tf.string),
            'image/encoded': tf.io.FixedLenFeature([], tf.string),
            'image/format': tf.io.FixedLenFeature([], tf.string),
            'image/object/bbox/xmin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),
            'image/object/bbox/xmax': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),
            'image/object/bbox/ymin': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),
            'image/object/bbox/ymax': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),
            'image/object/class/text': tf.io.FixedLenFeature([], tf.string),
            'image/object/class/label': tf.io.FixedLenFeature([], tf.int64),
        }
    )
    example = tf.io.parse_single_example(example, tfrecord_format)

    return example


def create_tf_example(sample: Sample, label_map: Dict[str, int],
                      class_weight: Dict[str, float]) -> tf.train.Example:
    # required to find immage dimensions
    with tf.io.gfile.GFile(sample.filepath, 'rb') as fid:
        encoded_jpg = fid.read()

    encoded_jpg_io = io.BytesIO(encoded_jpg)
    image = Image.open(encoded_jpg_io)
    width, height = image.size

    filename = sample.filepath.encode('utf8')
    image_format = os.path.splitext(sample.filepath)[-1].encode('utf8')

    feature = {
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(filename),
        'image/source_id': dataset_util.bytes_feature(filename),
        'image/encoded': dataset_util.bytes_feature(encoded_jpg),
        'image/format': dataset_util.bytes_feature(image_format),
        'image/object/bbox/xmin': dataset_util.float_list_feature([sample.x0 / width]),
        'image/object/bbox/xmax': dataset_util.float_list_feature([sample.x1 / width]),
        'image/object/bbox/ymin': dataset_util.float_list_feature([sample.y0 / height]),
        'image/object/bbox/ymax': dataset_util.float_list_feature([sample.y1 / height]),
        'image/object/class/text': dataset_util.bytes_list_feature(
            [sample.cls.encode('utf8')]),
        'image/object/class/label': dataset_util.int64_list_feature(
            [label_map[sample.cls]]),
        'image/object/weight': dataset_util.float_list_feature([class_weight[sample.cls]])
    }

    # convert everything to byte format for tfrecord
    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))
    return tf_example


def main(args: Namespace):
    label_map = label_map_util.get_label_map_dict(args.labelmap)

    with open(args.input) as f:
        df = pd.read_csv(f)

    if 'split' in df:
        splits = df['split'].unique()
        use_splits = True
    else:
        splits = ['samples']
        use_splits = False

    class_weights = generate_class_weights(df['cls'].to_list())
    log.info(f'Using class weights {class_weights}.')

    pbar = tqdm.tqdm(total=len(df))
    for split in splits:
        filename = path.join(args.output, split + '.tfrecord')

        if use_splits:
            # process each data split train/validation/test individually
            split_df = df[df['split'] == split]
        else:
            split_df = df

        with tf.io.TFRecordWriter(filename) as writer:
            for item in split_df.itertuples():
                tf_sample = create_tf_example(item, label_map, class_weights)
                writer.write(tf_sample.SerializeToString())

                pbar.update(1)

        log.info(f'Successfully created the {split} split TFRecord file: '
                 f'at {path.abspath(filename)}')


if __name__ == '__main__':
    from ..utils.logging import setup_logger

    parser = ArgumentParser()
    parser.add_argument('-l', '--labelmap', type=str,
                        help='Labelmap in pbtxt format.')
    parser.add_argument('-i', '--input', type=str,
                        help='Input .csv files generated by the preprocess_csv '
                             'script.')
    parser.add_argument('-o', '--output', type=str, default='.',
                        help='Directory to output TFRecord (.record) files.')
    parser.add_argument('-b', '--balance', action='store_true',
                        help='Balance the dataset by setting class weights')

    args = parser.parse_args()

    setup_logger()
    main(args)
